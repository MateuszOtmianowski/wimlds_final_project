{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Athena?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html) is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Serverless = easy to set up and use.\n",
    "- Fast.\n",
    "- You pay only for the queries you run.\n",
    "- Works with standard data formats like CSV, JSON, ORC, Avro and Parquet.\n",
    "- It is integrated with [AWS Glue](https://aws.amazon.com/glue/) (i.e. AWS ETL tool)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some queries with extremely large data might be nearly impossible to run (unless you find some hack).\n",
    "- It does not handle sorting large datasets well.\n",
    "- OpenCSV SerDe has troubles with NULL values in columns other than strings.\n",
    "- Data needs to be stored on S3, so you additionally pay for it's storage.\n",
    "- Might be costly (depending on the usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You are charged for the number of bytes scanned by Athena by every query you execute, with a 10MB minimum per query. \n",
    "TB of data scanned costs $5. \n",
    "- There are no charges for Data Definition Language (DDL) statements like CREATE/ALTER/DROP TABLE, statements for managing partitions, or failed queries.\n",
    "- Cancelled queries are charged based on the amount of data scanned.\n",
    "- When calculating the whole cost of the service it is worth remembering that you need to include the cost of S3 data storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Athena work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena uses two components:\n",
    "- [Presto](https://prestodb.io/docs/0.172/) as a distributed SQL engine to run queries, and \n",
    "- [Apache Hive](https://en.wikipedia.org/wiki/Apache_Hive) as [DDL](https://en.wikipedia.org/wiki/Data_definition_language) (to create, drop, and alter tables and partitions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check Athena out!\n",
    "Before we try Athena out, we need to have some data to work with on S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0 \n",
    "\n",
    "Upload titanic.csv file from your data folder to the separate bucket on s3. Remember that you need to give your bucket a unique name. Note somewhere the path to the bucket, as we will need it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data stored on S3 we can create a table out of it in Athena. To achieve this we need to do the following:\n",
    "\n",
    "1. Open Athena service in AWS.\n",
    "\n",
    "<img src='img/athena.png' width=900>\n",
    "\n",
    "\n",
    "2. Click \"Create table\", and then \"from S3 bucket data\".\n",
    "\n",
    "<img src='img/create_table.png' width=900>\n",
    "\n",
    "\n",
    "3. Fill in Step 1 with the following:\n",
    "    - Database = titanic_db\n",
    "    - Table Name = titanic\n",
    "    - Location of Input Data Set = location of your csv file ex. s3://titanic-data-2019-01/ (you need to put your's bucket name here)\n",
    "    \n",
    "    and click \"Next\".\n",
    "   \n",
    "   \n",
    "4. In Step 2 select \"CSV\".\n",
    "5. In Step 3 click on \"Bulk add columns\" and paste there the following:\n",
    "\n",
    "    ```\n",
    "    passengerid int, survived int, pclass smallint, name string, sex string, age double, \n",
    "    sibsp int, parch int, ticket string, fare double,cabin string, embarked string \n",
    "    ```\n",
    "   </br>\n",
    "   and then click \"Next\".\n",
    "   \n",
    "   \n",
    "6. In Step 4 click on \"Create table\".\n",
    "7. If you see a new table in the Tables section and you get \"Query successful.\" message in Results section of the console it means that you did it!\n",
    "\n",
    "<img src='img/success.png' width=900>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Athena translates what we did in the wizard into a query, that we could have used instead of using the wizard. The query looks like that:\n",
    "\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS titanic_db.titanic (\n",
    "  `passengerid` int,\n",
    "  `survived` int,\n",
    "  `pclass` smallint,\n",
    "  `name` string,\n",
    "  `sex` string,\n",
    "  `age` double,\n",
    "  `sibsp` int,\n",
    "  `parch` int,\n",
    "  `ticket` string,\n",
    "  `fare` double,\n",
    "  `cabin` string,\n",
    "  `embarked` string \n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "  'serialization.format' = ',',\n",
    "  'field.delim' = ','\n",
    ") LOCATION 's3://titanic-data-2019-01/' -- you need to put your bucket name here\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check it out by deleting the table that we have created either by clicking on the three dots next to the table name and then clicking on 'Delete table' or by executing `DROP TABLE titanic_db.titanic;`. Then paste `CRATE EXTERNAL TABLE...` query to the console and execute it. It should create a new table with Titanic's data as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have mentioned previously Athena uses Presto as an SQL engine. It's syntax is similar to other SQL dialects. Let's check how our data looks like with a simple query:\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM titanic_db.titanic\n",
    "LIMIT 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, it turns out our data is not correctly parsed into columns. In our `CREATE EXTERNAL TABLE` query the element that is responsible for parsing is called SerDe. LazySimpleSerDe is default one but as it turns out it does not handle double quotes. We need to use different SerDe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [Athena's documentation](https://docs.aws.amazon.com/athena/latest/ug/csv.html) and try to modify `CREATE EXTERNAL TABLE` query so it parses our data correctly. You can write your query here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- \n",
    "\n",
    "your query\n",
    "\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Functions in Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with our data a bit. Presto has an extensive [library of functions](https://prestodb.io/docs/0.172/functions.html) that allows us to do quite sophisticated manipulations with our data. We will look at a few examples in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [several functions](https://prestodb.io/docs/0.172/functions/regexp.html) in Presto for using regular expressions. Let's assume that we would like to extract a title from the name of every person. We can do this with the following query:\n",
    "\n",
    "```\n",
    "SELECT name, regexp_replace(regexp_extract(name, '[A-Za-z]+\\.'), '\\.') title\n",
    "FROM titanic_db.titanic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (5 min)\n",
    "\n",
    "Create a query that will, based a on the `cabin` column, create three new columns:\n",
    "- last_cabin: contains code for the last cabin in the sequence (ex. cabin = 'C23 C25 C27', then last_cabin = 'C27'). In case there is only one cabin code, show that code.\n",
    "- last_cabin_letter: contains just a letter from the last_cabin (ex. first_cabin = 'C23', then last_cabin_letter = 'C').\n",
    "- last_cabin_number: contains number(s) from the last_cabin (ex. first_cabin = 'C23', then last_cabin_number = '23').\n",
    "\n",
    "You can write your query here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- \n",
    "\n",
    "your query\n",
    "\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful group of functions, both in Presto and in other SQL dialects, are [window functions](https://prestodb.io/docs/0.172/functions/window.html). They allow us to calculate aggregate statistics on specific groups for every row of the data, without reducing the number of rows.\n",
    "\n",
    "In general, window functions consist of 3 elements:\n",
    "- a function that will be applied to a specific column,\n",
    "- partition specification, separating rows into different groups (so it works similarily to the GROUP BY statement),\n",
    "- ordering specification, which determines order in which input rows will be processed by the window function\n",
    "\n",
    "So the template for the most window functions looks like that:\n",
    "`function_name(column_name) OVER(PARTITION BY column_name, ... [ORDER BY column_name, ...])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's say we want to calculate the difference between the age of every person and the mean age for a given sex. Without a window function we could do this in the following way:\n",
    "\n",
    "```\n",
    "SELECT \n",
    "    t.name, \n",
    "    t.sex, \n",
    "    age, \n",
    "    a.avg_age, \n",
    "    CAST(t.age as DOUBLE) - a.avg_age age_diff_avg\n",
    "FROM titanic_db.titanic t\n",
    "    JOIN (\n",
    "        SELECT sex, avg(CAST(age AS DOUBLE)) avg_age\n",
    "        FROM titanic_db.titanic\n",
    "        WHERE age != ''\n",
    "        GROUP BY sex) a ON a.sex = t.sex\n",
    "WHERE t.age != ''\n",
    "```\n",
    "\n",
    "If we decide to use a window function, we can simplify this query to:\n",
    "\n",
    "```\n",
    "SELECT \n",
    "    name, \n",
    "    sex, \n",
    "    age, \n",
    "    AVG(CAST(age as DOUBLE)) OVER (PARTITION BY sex) avg_age, \n",
    "    CAST(age AS DOUBLE) - AVG(CAST(age as DOUBLE)) OVER (PARTITION BY sex)\n",
    "FROM titanic_db.titanic\n",
    "WHERE age != ''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (5 min)\n",
    "Assuming the price of ticket should reflect port of embarkation (`embarked`) and class (`class`), write a query that will show the number of passangers that overpaid for their tickets (i.e. paid more than average for their port of embarkation and class). You can write your query here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- \n",
    "\n",
    "your query\n",
    "\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presto has also pretty useful set of [functions](https://prestodb.io/docs/0.172/functions/array.html) to work with arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we would like to put our data into more concise form, and instead of having a row for every passanger, we would like to have a cabin in every row and in columns data concerning passangers of those cabins. We could do it with arrays:\n",
    "```\n",
    "SELECT \n",
    "    cabin, \n",
    "    array_agg(name) names,\n",
    "    array_agg(sex) sexes,\n",
    "    array_agg(CAST(age as double)) ages\n",
    "FROM titanic_db.titanic\n",
    "WHERE age != '' and cabin != ''\n",
    "GROUP BY cabin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then do some calculations on those arrays like calculate the age of the oldest person that was travelling in each cabin:\n",
    "```\n",
    "SELECT cabin, ages, array_max(ages) max_age\n",
    "FROM (\n",
    "    SELECT \n",
    "        cabin, \n",
    "        array_agg(name) names,\n",
    "        array_agg(sex) sexes,\n",
    "        array_agg(CAST(age as double)) ages\n",
    "    FROM titanic_db.titanic\n",
    "    WHERE age != '' and cabin != ''\n",
    "    GROUP BY cabin)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (5 min)\n",
    "\n",
    "For every cabin count how many underage passangers were occupying it. \n",
    "\n",
    "You can write your query here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- \n",
    "\n",
    "your query\n",
    "\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tables in Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [October 2018](https://docs.aws.amazon.com/athena/latest/ug/release-note-2018-10-10.html) Amazon introduced a feature to create a new table based on the result of another query. Before that, creating tables involved a lot of hacks, but now it is pretty simple. We just need to use `CREATE TABLE table_name AS query` statement and a new table with results of the query will be created for us:\n",
    "```\n",
    "CREATE TABLE titanic_females AS \n",
    "SELECT * \n",
    "FROM titanic_db.titanic \n",
    "WHERE sex = 'female'\n",
    "```\n",
    "Data is by default in the [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) format in the location: \n",
    "\n",
    "`s3://aws-athena-query-results-<account>-<region>/<query-name-or-unsaved>/<year>/<month/<date>/<query-id>/` \n",
    "\n",
    "However, it would be prudent to change this default location, to organise your data in more reasonable manner (e.g. separate bucket/prefix for every table). A query result location can be changed in Athena settings. Keep in mind that you need to change it every time you run a query, to not to populate your table with unwanted data. As you can imagine it is quite burdersome, so it might be better to do it programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Athena from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting query results into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pyathena](https://pypi.org/project/PyAthena/) is the package that works well with Athena and is quite easy to use. It has a `connect` function in which you can specify security credentials, staging dir (i.e. where results of your query will be saved), and region that you are set up in. If you don't specify security credentials it will look for default credentials in `.aws/credentials` on your machine (for Unix systems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyathena import connect\n",
    "\n",
    "conn = connect(s3_staging_dir='s3://aws-athena-query-results-920137764546-us-east-1/', # put default query folder here\n",
    "               region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`connect` function creates an instance of `pyathena.connection.Connection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyathena.connection.Connection at 0x10ce97a20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this object, for example in pandas `read_sql` function, to download results of a query as pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerid</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td></td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passengerid  survived  pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                name     sex   age  sibsp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male            0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   parch            ticket     fare cabin embarked  \n",
       "0      0         A/5 21171   7.2500              S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250              S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500              S  \n",
       "5      0            330877   8.4583              Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750              S  \n",
       "8      2            347742  11.1333              S  \n",
       "9      0            237736  30.0708              C  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM titanic_db.titanic LIMIT 10\", conn)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method is ok for smaller data, but it usually takes quite a long time to download larger data sets. In those instances it usually better to execute query and to download the resulting csv file directly from s3. We can use the same connect object to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connect(s3_staging_dir='s3://titanic-females/', # insert your bucket here\n",
    "                 region_name='us-east-1').cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM titanic_db.titanic WHERE sex = 'female'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('titanic-females')\n",
    "[bucket.download_file(o.key, 'data/' + o.key) for o in bucket.objects.all() \n",
    " if 'csv' in o.key and 'metadata' not in o.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use boto3's Athena client, which has a `start_query_execution` method that executes query and returns metada of the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('athena')\n",
    "query = \"SELECT * FROM titanic_db.titanic WHERE sex = 'female'\"\n",
    "response = client.start_query_execution(\n",
    "    QueryString=query,\n",
    "    ResultConfiguration={\n",
    "        'OutputLocation': 's3://titanic-females/', # insert your bucket here\n",
    "        'EncryptionConfiguration': {\n",
    "            'EncryptionOption': 'SSE_S3',\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metadata contains, among others, the QueryExecutionId which is also the name of the csv file that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QueryExecutionId': '35807c98-f98f-41b9-91b5-d2d8f065b73f',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
       "   'content-length': '59',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 16 Jan 2019 13:18:10 GMT',\n",
       "   'x-amzn-requestid': '65afe49a-7be1-43de-8a9b-30047424705b'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': '65afe49a-7be1-43de-8a9b-30047424705b',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this id to download the csv file with the result directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('titanic-females')\n",
    "bucket.download_file(response['QueryExecutionId']+'.csv', 'data/query_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tables using Python\n",
    "We can create tables directly from Python using `CREATE TABLE AS` queries and the same `cursor.execute` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connect(s3_staging_dir='s3://titanic-females-table/', # insert your bucket here\n",
    "                 region_name='us-east-1').cursor()\n",
    "create_tbl_query = \"CREATE TABLE titanic_females AS SELECT * FROM titanic_db.titanic WHERE sex = 'female'\"\n",
    "cursor.execute(create_tbl_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has created a table in default schema. To create it in `titanic_db` schema we can again use `start_query_execution` with QueryExecutionContext set to the proper schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tbl_resp = client.start_query_execution(\n",
    "    QueryString=create_tbl_query,\n",
    "    QueryExecutionContext={\n",
    "        'Database': 'titanic_db'\n",
    "    },\n",
    "    ResultConfiguration={\n",
    "        'OutputLocation': 's3://titanic-females-table/', # insert your bucket here\n",
    "        'EncryptionConfiguration': {\n",
    "            'EncryptionOption': 'SSE_S3',\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have only scratched the surface here, but a few things to keep in mind are:\n",
    "- Athena is AWS tool for analysing data stored on S3 using SQL syntax.\n",
    "- It uses Presto as na SQL engine, which is quite capable.\n",
    "- You can work with Athena using AWS console, Python or data base tools like DBeaver."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
